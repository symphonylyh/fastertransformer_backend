[decoder]
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = False
is_encoder_decoder = False
is_decoder = False
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
suppress_tokens = None
begin_suppress_tokens = None
architectures = None
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}
tokenizer_class = None
prefix = None
bos_token_id = 0
pad_token_id = 1
eos_token_id = 2
sep_token_id = None
decoder_start_token_id = 2
task_specific_params = None
problem_type = None
_name_or_path = 
transformers_version = 4.28.1
gradient_checkpointing = True
vocab_size = 252526
d_model = 256
embed_dim = None
encoder_ffn_dim = 4096
encoder_layers = 12
num_hidden_layers = 12
encoder_attention_heads = 16
encoder_layerdrop = 0.0
decoder_layerdrop = 0.0
decoder_ffn_dim = 1024
decoder_layers = 12
decoder_attention_heads = 4
max_position_embeddings = 1024
init_std = 0.02
activation_function = gelu
scale_embedding = False
normalize_embedding = True
normalize_before = False
add_final_layer_norm = False
add_bias_logits = False
static_position_embeddings = False
attention_dropout = 0.0
activation_dropout = 0.0
dropout = 0.1
classifier_dropout = 0.0
extra_pos_embeddings = 2
force_bos_token_to_be_generated = False
do_blenderbot_90_layernorm = False
use_cache = False
fp32_cast_query_key = None
share_embeddings = True
relative_attention_num_buckets = 32
relative_attention_max_distance = 128
scale_init_for_embeddings = True
layer_norm_eps = 1e-12
use_megatron_softmax = False
use_activation_offloading = False
activation_offloading_max_copy_events = 9
activation_offloading_max_num_prefetch = 2
position_bias_per_layer = True
scale_attention = True
add_lm_head_bias = False
use_flash_attention = False
use_relative_attention_position_embedding = True
use_alibi_position_embedding = False
max_alibi_positions = 2048
mup_scale = True
model_type = bart
weight_data_type = fp16

[structure]
agm_with_bias = true
activation_function = gelu
position_embedding_type = relative
scale_attention = True
mup_scale = True
pad_embedding = False
