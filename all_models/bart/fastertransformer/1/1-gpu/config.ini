[encoder]
vocab_size = 250054
d_model = 1024
encoder_ffn_dim = 4096
encoder_layers = 12
encoder_attention_heads = 16
weight_data_type = fp32
architectures = ['MBartForConditionalGeneration']
_name_or_path = /haohangh/data/bart-models/mbart-large-50/
transformers_version = 4.20.1
model_type = bart

[decoder]
vocab_size = 250054
d_model = 1024
decoder_ffn_dim = 4096
decoder_layers = 12
decoder_attention_heads = 16
tie_word_embeddings = False
bos_token_id = None
pad_token_id = 1
eos_token_id = 2
sep_token_id = None
decoder_start_token_id = 2
weight_data_type = fp32
architectures = ['MBartForConditionalGeneration']
_name_or_path = /haohangh/data/bart-models/mbart-large-50/
transformers_version = 4.20.1
model_type = bart

[structure]
bart_with_bias = true
mbart = true
use_gated_activation = false
activation_function = gelu
position_embedding_type = absolute
